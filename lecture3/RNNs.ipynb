{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks and their modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data path initialization\n",
    "BASE_DIR = '../'\n",
    "TEXT_DATA_DIR = BASE_DIR + 'data/'\n",
    "TEXT_DATA_FILE = \"movie_reviews.csv\"\n",
    "HEADER = True\n",
    "\n",
    "# parameters initialization\n",
    "VALIDATION_SPLIT = 0.1\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # function for loading data\n",
    "    x = []\n",
    "    y = []\n",
    "    with open(os.path.join(TEXT_DATA_DIR, TEXT_DATA_FILE), \"r\", encoding=\"utf-8\") as f:\n",
    "        if HEADER:\n",
    "            _ = next(f)\n",
    "        for line in f:\n",
    "            temp_y, temp_x = line.rstrip(\"\\n\").split(\",\", 1)\n",
    "            x.append(temp_x.replace(\"'\", \"\"))\n",
    "            y.append(temp_y)\n",
    "    return x, y\n",
    "\n",
    "data, labels = load_data()\n",
    "labels = np.asarray(labels, dtype='int8')\n",
    "\n",
    "# spliting our original data on train and validation sets\n",
    "data_train, data_val, labels_train, labels_val = \\\n",
    "    train_test_split(data, np.asarray(labels, dtype='int8'),\n",
    "                     test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop we will use pre-trained vectors, since our text body is not large enough to train own embeddings. For the calculation speed, we will use [glove vectors of dimension 50](http://nlp.stanford.edu/data/glove.6B.zip).\n",
    "\n",
    "Convert words to vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      " \"Filled with sentimentality, pretensions, unfulfilled ambitions and a host of dull characters faced with life threatening problems that verge on the ludicrous.\"\n",
      "Sentence in indexes:\n",
      " [958, 13, 4032, 8229, 6794, 3, 2, 2910, 4, 676, 95, 2286, 13, 107, 4109, 716, 11, 7462, 19, 1, 2513]\n",
      "Sentence fitted to max length:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  958   13 4032 8229 6794    3    2 2910    4  676   95\n",
      " 2286   13  107 4109  716   11 7462   19    1 2513]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# инициализируем параметры словаря и эмбеддингов\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "print(\"Original sentence:\\n\", data_train[0])\n",
    "\n",
    "# create a dictionary with Tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.!\"')\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "\n",
    "# replacing words with their indexes from our dictionary\n",
    "X_train = tokenizer.texts_to_sequences(data_train)\n",
    "X_test = tokenizer.texts_to_sequences(data_test)\n",
    "\n",
    "print(\"Sentence in indexes:\\n\", X_train[0])\n",
    "\n",
    "# обрезаем каждое предложение приводим к нужной длинне\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"Sentence fitted to max length:\\n\", X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "The `Tokenizer` object contains all the information about our dictionary. You should find the index of the word \"nice\" and how many times it was found in our train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nice' index – 0.\n",
      "The word 'nice' was found 0 times.\n"
     ]
    }
   ],
   "source": [
    "# Replace 0 with a right answer\n",
    "print(\"'nice' index – {}.\".format(0))\n",
    "print(\"The word 'nice' was found {} times.\".format(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Here is a right answer</summary>\n",
    "    <pre>\n",
    "      <code>\n",
    "  print(\"'nice' index – {}.\".format(tokenizer.word_index['nice']))\n",
    "  print(\"The word 'nice' was found {} times.\".format(tokenizer.word_counts['nice']))\n",
    "      </code>\n",
    "    </pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with vector representation: 400000\n"
     ]
    }
   ],
   "source": [
    "# path to embeddings file\n",
    "EMBEDDINGS_DIR = BASE_DIR + 'embeddings'\n",
    "EMBEDDINGS_FILE = 'glove.6B.50d.txt'\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "# choose only 10000 words from our dictionary\n",
    "first_10000 = {k: v for k, v in tokenizer.word_index.items() if v < 10000}\n",
    "\n",
    "# upload embeddings\n",
    "embeddings = {}\n",
    "with zipfile.ZipFile(os.path.join(EMBEDDINGS_DIR, EMBEDDINGS_FILE+'.zip')) as myzip:\n",
    "    with myzip.open(EMBEDDINGS_FILE) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0].decode('UTF-8')\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        del values, word, coefs, line\n",
    "print(\"Number of words with vector representation:\", len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Find how many of the most common 10,000 words from our dictionary are not in the embedding dictionary. How can this amount be reduced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Here is a right answer</summary>\n",
    "      <pre>\n",
    "            <code>\n",
    "            len(set(first_10000.keys()).difference(embeddings.keys()))\n",
    "            </code>\n",
    "            Add more filters to the argument `filters` of `Tokenizer` object.\n",
    "      </pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embeddings matrix where each row is word index\n",
    "\n",
    "embedding_matrix = np.zeros((tokenizer.num_words, EMBEDDING_DIM))\n",
    "for word, i in first_10000.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks (RNN)\n",
    "\n",
    "The simplest RNN formulation that is sensitive to the ordering of elements in the sequence is known as an Elman Network or Simple-RNN (S-RNN). The S-RNN was proposed by [Elman](http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract;jsessionid=BC63955BD05165200EF5A1117E134554.f02t04) and explored for use in language modeling by [Mikolov](http://www.fit.vutbr.cz/~imikolov/rnnlm/google.pdf).\n",
    "\n",
    "Recurrent neural networks help to grasp / understand the regularity, which depends on time or order. For example, when we try to classify an episode from a movie, it is important for us to know what was a couple of episodes earlier, or to understand the meaning of a certain word, we need to know the context that was before it.\n",
    "\n",
    "A simple recurrent neural network has the following mathematical representation:<br><br>\n",
    "$$\\large h_t = \\phi(Wx_t + Uh_{t-1})$$<br>\n",
    "$$\\large y = Vh_t$$\n",
    "\n",
    "A formula illustration:\n",
    "<img src=\"http://i.imgur.com/ifQrKRR.png\" alt=\"rnn\" style=\"width: 700px;\"/>\n",
    "\n",
    "It is easy to see that an unrolled RNN is just a very deep neural network (or rather, a very large computation graph with somewhat complex nodes), in which the same parameters\n",
    "are shared across many parts of the computation, and additional input is added at various layers. To train an RNN network, then, all we need to do is to create the unrolled computation graph for a given input sequence, add a loss node to the unrolled graph, and then use the backward (backpropagation) algorithm to compute the gradients with respect to that loss. This procedure is referred to in the RNN literature as backpropagation through time ([BPTT](http://ieeexplore.ieee.org/document/58337/))\n",
    "\n",
    "## Task 3\n",
    "\n",
    "Draw an unrolled Simple Recurrent Network at time steps from k-2 to k.\n",
    "\n",
    "<details>\n",
    "  <summary>Here is a right answer</summary>\n",
    "     <img src=\"https://photos-4.dropbox.com/t/2/AACkZO6XE2d62AQPmvsvtdX_kX5XxKtTbSFITuNHyzCZJA/12/533843084/png/32x32/1/_/1/2/unrolled_rnn.png/EKyZ0aIEGN0iIAIoAg/rJwBiWaoT5zQ4DS_oaXRKnJuGf7E6mJvCE1S3E-Iebg?size=2048x1536&size_mode=3\" alt=\"unrolled_rnn\" style=\"width: 700px;\"/>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 515,201\n",
      "Trainable params: 15,201\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "NAME = \"simple_rnn\"\n",
    "\n",
    "# embedding layer initialization\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# callbacks initialization\n",
    "# automatic generation of learning curves\n",
    "callback_1 = TensorBoard(log_dir='./logs/logs_{}'.format(NAME), histogram_freq=0,\n",
    "                             write_graph=False, write_images=False)\n",
    "# stop training model if accuracy does not increase more than five epochs\n",
    "callback_2 = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "# best model saving\n",
    "callback_3 = ModelCheckpoint(\"../models/model_{}.hdf5\".format(NAME), monitor='val_acc',\n",
    "                                 save_best_only=True, verbose=1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you train this model, then the accuracy on the training sample will be **77.13%**, and on validation - **75.60%**.\n",
    "\n",
    "The S-RNN is hard to train effectively because of the vanishing gradients problem ([Pascanu et al., 2012](https://arxiv.org/abs/1211.5063)). Error signals (gradients) in later steps in the sequence diminish quickly in the backpropagation process, and do not reach earlier input signals, making it hard for the S-RNN to capture long-range dependencies. Gating-based architectures, such as the LSTM ([Hochreiter and Schmidhuber, 1997](http://www.bioinf.jku.at/publications/older/2604.pdf)) and the GRU ([Cho et al., 2014b](https://arxiv.org/abs/1406.1078)) are designed to solve this deficiency.\n",
    "\n",
    "Consider the RNN as a general purpose computing device, where the state $s_i$ represents a finite memory. Each application of the function R reads in an input $x_{i+1}$, reads in the current memory $s_i$ , operates on them in some way, and writes the result into memory, resulting in a new memory state $s_{i+1}$. Viewed this way, an apparent problem with the S-RNN architecture is that the memory access is not controlled. At each step of the computation, the entire memory state is read, and the entire memory state is written.\n",
    "\n",
    "***\n",
    "\n",
    "## Long short-term memory (LSTM)\n",
    "\n",
    "LSTM has a number of advantages over a simple recurrent neural network. LSTM is able to store the necessary information about a certain object and does not pay attention to not actual information. For example, a scene without a mention of the main character will not change information about him and it will focus with the mention.\n",
    "\n",
    "- ** Adding a forgetting mechanism. ** If the episode ends, for example, the model should forget the current location, time of day and forget any information about the particular scene. However, if the character dies in the episode, the network must continue to remember that he is no longer alive. Thus, we want the model to study a separate forget / remember mechanism: when new input data appear, it must know what facts to keep or throw away.\n",
    "\n",
    "- ** Adding a saving mechanism. ** When the model sees a new episode, it needs to decide whether it is worth using and storing any information about it. You saw some new meme, but who cares?\n",
    "\n",
    "- Therefore, when a new entry comes in, the model first forgets the long-term information that it decides that it is no longer needed. Then it learns which part of the new data should be used and stores them in its long-term memory.\n",
    "\n",
    "- ** Focusing long-term memory into working memory. ** Finally, the model should find out which parts of its long-term memory are now useful. For example, the age of the hero can be useful information to keep in the long term (children are more likely to crawl, adults are more likely to work), but probably does not matter if he is not in the current scene. So, instead of using full long-term memory all the time, he learns which parts to focus on.\n",
    "\n",
    "The advantage of LSTM over RNN is that RNN can overwrite its memory at each time step in a fairly uncontrolled fashion, but LSTM is more flexible and can store long-term information, focusing on the right parts of it.\n",
    "\n",
    "Now consider this all from the side of mathematics.\n",
    "\n",
    "Let's start with ** long-term memory **. First, we need to know which fragments of long-term memory ** continue to remember and which to forget **, so we need to understand, based on the new input and our working memory, what part of the long-term memory should be keep.\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\" alt=\"forget\" style=\"width: 600px;\"/>\n",
    "\n",
    "Thus, we get the value between 0 and 1 as an output, where 0 - we forget everything, 1 - we remember everything.\n",
    "\n",
    "Now it is necessary to decide what new information to remember and what part of it to add to the long-term memory:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\" alt=\"add\" style=\"width: 600px;\"/>\n",
    "\n",
    "This step consists of two parts: the first is what kind of information we want to update ($ \\large i_t $); And the second - the candidate addition to long-term memory ( $\\large \\tilde{C_t}$).\n",
    "\n",
    "Now we need to update our long-term memory:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\" alt=\"add\" style=\"width: 600px;\"/>\n",
    "\n",
    "After we have updated our long-term memory, we need to focus on the right information for a specific example.\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" alt=\"add\" style=\"width: 600px;\"/>\n",
    "\n",
    "In simple words, if we focus on some information, then the activation of the sigmoid returns 1, and if some information is not needed now, then we return 0.\n",
    "\n",
    "Let's look at a toy example for better understanding. We will train on Pokemons :)\n",
    "\n",
    "What \"thinks\" a fully connected neural network:\n",
    "<img src=\"http://i.imgur.com/cOGzJxk.png\" alt=\"pokemon_nn\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "What \"thinks\" is a simple recurrent network:\n",
    "<img src=\"http://i.imgur.com/PnWiSCf.png\" alt=\"pokemon_rnn\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "As can be seen from the picture, the recurrent network remembers what happened a couple of seconds ago and can roughly understand what caused the appearance of water in the next episode.\n",
    "\n",
    "What \"thinks\" LSTM:\n",
    "<img src=\"http://i.imgur.com/EGZIUuc.png\" alt=\"pokemon_lstm\" style=\"heigh: 100px;\"/>\n",
    "\n",
    "LSTM recalls what happened in the previous episode, also recall long-term memory and focuses only on the right information for a specific episode.\n",
    "\n",
    "Sources:\n",
    "1. [Exploring LSTMs](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "2. [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "3. [Neural Network Methods for Natural Language Processing by Yoav Goldberg](http://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 560,501\n",
      "Trainable params: 60,501\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "# инициализируем слой эмбеддингов\n",
    "NAME = \"simple_lstm\"\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters in a simple LSTM are almost 4 times larger, the mathematical exposure to this fact can be seen from the formulas above.\n",
    "\n",
    "When replacing a simple RNN with LSTM, the accuracy in the training sample increased to **82.49%**, and on validation - **77.71%**.\n",
    "\n",
    "***\n",
    "\n",
    "# Gated Recurrent Unit (GRU)\n",
    "\n",
    "The LSTM architecture is very effective, but also quite complicated. The complexity of the system makes it hard to analyze, and also computationally expensive to work with. The gated recurrent unit (GRU) was introduced by Cho et al. as an alternative to the LSTM. It was\n",
    "subsequently shown to perform comparably to the LSTM on several (non-textual) datasets. Like the LSTM, the GRU is also based on a gating mechanism, but with substantially fewer gates and without a separate memory component.\n",
    "\n",
    "It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes.\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\" alt=\"pokemon_lstm\" style=\"heigh: 100px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               45300     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 545,401\n",
      "Trainable params: 45,401\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "# инициализируем слой эмбеддингов\n",
    "NAME = \"simple_lstm\"\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(100))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are fewer number of parameters in a simple GRU than in LSTM, so our model should less tend to overfitting on small datasets.\n",
    "\n",
    "When replacing LSTM with GRU, the accuracy in the training sample still almost the same – **82.38%** and increased on validation - **78.49%**.\n",
    "\n",
    "## Task 4\n",
    "\n",
    "Now let's try to improve our simple LSTM model by adding the following modifications (the number of parameters should not change):\n",
    "<details>\n",
    "  <summary>Here is a right answer!</summary>\n",
    "      <pre>\n",
    "          1. Dropouts (Reduce overfitting).\n",
    "          2. Masking (This parameter should be added to the initialization of embedding, so that the loss function does not take into account 0, when our input is less than the maximum length).\n",
    "          3. Regularization(This approach often works, but not in the case of LSTM. l1 / l2 regularization is often used to prevent the explosion of gradients, but the LSTM cell is constructed in such a way that there are no explosions, so the use of l1 / l2 regularization is impractical and worsens the results).\n",
    "      </pre>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 560,501\n",
      "Trainable params: 60,501\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# инициализируем слой эмбеддингов\n",
    "NAME = \"modified_lstm\"\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)\n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model showed itself, as we expected, better: on the training sample - **78.99%**, on the validation sample - **79.81%**.\n",
    "\n",
    "***\n",
    "\n",
    "# Bidirectional and stacked LSTM\n",
    "\n",
    "We can create more complicated models using RNNs. One of them are bidirectional and stacked LSTMs. In simple LSTM we go only from left to right, but in bidirectional we create to independent LSTMs: in the first we go as usual from left to right, and in another one – from right to left. Then concat output sequence before fully connected layer.\n",
    "\n",
    "<img src=\"http://doc.paddlepaddle.org/release_doc/0.9.0/doc/_images/bi_lstm1.jpg\" alt=\"pokemon_lstm\" style=\"heigh: 100px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               120800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 621,001\n",
      "Trainable params: 121,001\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "# инициализируем слой эмбеддингов\n",
    "NAME = \"bidirectional_lstm\"\n",
    "\n",
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)\n",
    "                            \n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got twice as many parameters as expected. Train accuracy is **81.55%**, validation – **80.10%**. The accuracy is higher than for simple lstm. In big datasets it should work even more better.\n",
    "\n",
    "***\n",
    "\n",
    "Stacked LSTM is another good modification. In that way we want to learn higher-level temporal representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 40, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 640,901\n",
      "Trainable params: 140,901\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(tokenizer.num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, recurrent_dropout=0.1, dropout=0.1, return_sequences=True))\n",
    "model.add(LSTM(100, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "#model.fit(X_train, labels_train, validation_data=[X_test, labels_test], \n",
    "#          batch_size=1024, epochs=100, callbacks=[callback_1, callback_2, callback_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked LSTM performes a little bit worse than bidirectional LSTM: train – **79.09%**, validation – **79.80%**.\n",
    "\n",
    "As you can see, models performance is very close, so it worth to try different architectures and regularization techniques to get the best accuracy on a specific task.\n",
    "\n",
    "These approaches are basic in applying recurrent neural networks for the sentiment analysis problem. Improve accuracy can be the following modifications:\n",
    "\n",
    "- size of embeddings;\n",
    "- the number of words in the dictionary;\n",
    "- the maximum long sentence;\n",
    "- modification of the network architecture;\n",
    "- ensembles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ucu-sentiment]",
   "language": "python",
   "name": "conda-env-ucu-sentiment-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
